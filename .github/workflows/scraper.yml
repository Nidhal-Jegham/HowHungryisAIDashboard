name: Run Scraper

on:
  schedule:
    - cron: "0 10 * * *"
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Install Chrome & Matching ChromeDriver
        run: |
          set -eux  # fail fast, echo commands
      
          # 1) Update and install Chrome non-interactively (silencing needrestart)
          wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo apt-get update
          sudo DEBIAN_FRONTEND=noninteractive \
               NEEDRESTART_MODE=a \
               apt-get install -y ./google-chrome-stable_current_amd64.deb
      
          # 2) Grab just the major version number (e.g. "138")
          CHROME_MAJOR=$(google-chrome-stable --product-version | cut -d. -f1)
          echo "Detected Chrome major version: $CHROME_MAJOR"
      
          # 3) Fetch the matching ChromeDriver version
          CHROMEDRIVER_VERSION=$(wget -qO- \
            "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_MAJOR}")
          echo "Matching ChromeDriver version: $CHROMEDRIVER_VERSION"
      
          # 4) Download & install it
          wget -q "https://chromedriver.storage.googleapis.com/${CHROMEDRIVER_VERSION}/chromedriver_linux64.zip"
          unzip -q chromedriver_linux64.zip
          chmod +x chromedriver
          sudo mv chromedriver /usr/local/bin/
      
          # sanity check
          chromedriver --version

      








      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install Python packages
        run: pip install -r requirements.txt

      - name: Run scraper
        run: |
          mkdir -p data
          xvfb-run python ArtificialAnalysisScraping.py
          mv artificialanalysis_clean*.csv data/
      - name: Check filenames exactly
        run: ls -lh data/


      - name: Run cleaning script
        run: |
          mkdir -p output
          python Data_Cleaning_Final.py
          mv artificialanalysis*.csv output/



      - name: Commit CSVs to repo
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "actions@github.com"
          git add data/*.csv
          git add output/*.csv

          git commit -m "Auto-update scraped and cleaned data [skip ci]" || echo "No changes to commit"
          git push
        env:
          # Required for permission to push back to the repo
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
