name: Run Scraper

on:
  schedule:
    - cron: "0 10 * * *"
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Install Chrome & ChromeDriver via Chrome-for-Testing API
        run: |
          set -eux
      
          # 1) Install prerequisites
          sudo apt-get update
          sudo DEBIAN_FRONTEND=noninteractive apt-get install -y wget unzip jq
      
          # 2) Install latest Google Chrome (silencing needrestart)
          wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo DEBIAN_FRONTEND=noninteractive NEEDRESTART_MODE=a \
               apt-get install -y ./google-chrome-stable_current_amd64.deb
      
          # 3) Fetch Chrome-for-Testing manifest & grab the Linux64 chromedriver URL
          CFT_JSON="https://googlechromelabs.github.io/chrome-for-testing/last-known-good-versions-with-downloads.json"
          CFT_URL=$(curl -sL "$CFT_JSON" \
            | jq -r '.channels.Stable.downloads.chromedriver[]
                       | select(.platform=="linux64")
                       | .url')
          echo "Downloading ChromeDriver from: $CFT_URL"
      
          # 4) Download & extract just the binary into CWD
          wget -qO chromedriver.zip "$CFT_URL"
          unzip -qj chromedriver.zip -d .
          chmod +x chromedriver
          sudo mv chromedriver /usr/local/bin/
      
          # 5) Verify installation
          chromedriver --version



      








      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install Python packages
        run: pip install -r requirements.txt

      - name: Run scraper
        run: |
          mkdir -p data
          xvfb-run python ArtificialAnalysisScraping.py
          mv artificialanalysis_clean*.csv data/
      - name: Check filenames exactly
        run: ls -lh data/


      - name: Run cleaning script
        run: |
          mkdir -p output
          python Data_Cleaning_Final.py
          mv artificialanalysis*.csv output/



      - name: Commit CSVs to repo
        run: |
          git config --global user.name "GitHub Action"
          git config --global user.email "actions@github.com"
          git add data/*.csv
          git add output/*.csv

          git commit -m "Auto-update scraped and cleaned data [skip ci]" || echo "No changes to commit"
          git push
        env:
          # Required for permission to push back to the repo
          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}
