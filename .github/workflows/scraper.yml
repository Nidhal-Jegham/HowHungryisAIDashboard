name: Run Scraper

on:
  schedule:
    - cron: "0 10 * * *"
  workflow_dispatch:

jobs:
  scrape:
    runs-on: ubuntu-22.04

    steps:
      - name: Checkout repo
        uses: actions/checkout@v3

      - name: Install Chrome & ChromeDriver via Chrome-for-Testing API
        run: |
          set -eux
      
          sudo apt-get update
          sudo DEBIAN_FRONTEND=noninteractive apt-get install -y wget unzip jq
      
          wget -q https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb
          sudo DEBIAN_FRONTEND=noninteractive NEEDRESTART_MODE=a \
               apt-get install -y ./google-chrome-stable_current_amd64.deb
      
          CFT_JSON="https://googlechromelabs.github.io/chrome-for-testing/last-known-good-versions-with-downloads.json"
          CFT_URL=$(curl -sL "$CFT_JSON" \
            | jq -r '.channels.Stable.downloads.chromedriver[]
                       | select(.platform=="linux64")
                       | .url')
          echo "Downloading ChromeDriver from: $CFT_URL"
      
          wget -qO chromedriver.zip "$CFT_URL"
          unzip -qj chromedriver.zip -d .
          chmod +x chromedriver
          sudo mv chromedriver /usr/local/bin/
      
          chromedriver --version



      








      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.10"

      - name: Install Python packages
        run: pip install -r requirements.txt

      - name: Run scraper
        run: |
          mkdir -p data
          xvfb-run python ArtificialAnalysisScraping.py
          mv artificialanalysis_clean*.csv data/
      - name: Check filenames exactly
        run: ls -lh data/


      - name: Run cleaning script
        run: |
          set -e
          mkdir -p output
          python Data_Cleaning_Final.py
      
          # Move only if present (first run safety)
          test -f artificialanalysis_environmental.csv && mv artificialanalysis_environmental.csv output/
          ls artificialanalysis_environmental_*.csv >/dev/null 2>&1 && mv artificialanalysis_environmental_*.csv output/ || true
          test -f artificialanalysis_environmental_history.csv && mv artificialanalysis_environmental_history.csv output/ || true
      
      - name: Commit CSVs to repo
        run: |
            set -e
            git config --global user.name "GitHub Action"
            git config --global user.email "actions@github.com"
            git config --global --add safe.directory "$GITHUB_WORKSPACE"
        
            # Add only if they exist
            git add data/*.csv 2>/dev/null || true
            test -f output/artificialanalysis_environmental.csv && git add output/artificialanalysis_environmental.csv || true
            ls output/artificialanalysis_environmental_*.csv >/dev/null 2>&1 && git add output/artificialanalysis_environmental_*.csv || true
            test -f output/artificialanalysis_environmental_history.csv && git add output/artificialanalysis_environmental_history.csv || true
        
            git commit -m "Auto-update scraped and cleaned data [skip ci]" || echo "No changes to commit"
        
            # Avoid non-fast-forward issues
            git pull --rebase || true
            git push
        env:
            GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}

